# 神经网络基础 (Neural Network Fundamentals)

## 1. 什么是神经网络？

神经网络 (Neural Network) 是一个具有相连节点层的计算模型，其分层结构与大脑中的神经元网络结构相似。它通过大量具有链接权重的网络层来不断地对输入进行抽象和特征提取。

## 2. 核心组成部分

典型的人工神经网络具有以下三个部分：

- **结构 (Architecture)**：指定了网络中的变量（如权重和激励值）及其拓扑关系。这定义了网络的框架，比如有多少层，每层有多少个神经元，以及它们之间如何连接。

- **激励函数 (Activation Rule)**：也称为激活函数，它决定了神经元的输出。激励函数通常依赖于网络中的权重和输入的加权和。

- **学习规则 (Learning Rule)**：指定了网络中的权重如何根据经验（数据）进行调整。这个规则的目标是让网络能够从数据中学习，最常见的学习规则是梯度下降法和反向传播算法。

## 3. 工作流程示例

以手写数字识别为例：

1. 一组输入神经元接收图像数据并被激活。
2. 这些神经元的激励值被加权、求和，并通过一个激励函数。
3. 处理后的结果被传递到下一层神经元。
4. 这个过程在网络中逐层重复，直到信号到达输出层。
5. 最终，输出层神经元的激励值决定了模型识别出的结果是哪个数字。

---

## 4. 感知机 (Perceptron)

感知机是由Frank Rosenblatt在1957年发明的一种早期人工神经网络。它可以被看作是**最简单形式的前馈神经网络**，本质上是一种**二元线性分类器**。

### 数学形式

感知机接收多个输入，将它们与权重相乘后求和，再加上一个偏置项 `b`，最后通过一个激活函数（通常是阶跃函数，如 `sign`）来得到输出。

$$f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^{d} w_i x_i + b\right) = \text{sign}(\mathbf{w}^T \mathbf{x})$$

- **优点**：简单、易于理解。
- **缺点**：只能解决线性可分问题。对于像异或 (XOR) 这样的非线性问题则无能为力。

## 5. 多层感知机 (Multilayer Perceptron, MLP)

多层感知机 (MLP) 是一种前向结构的人工神经网络，它由多个感知机层堆叠而成，能够解决单层感知机无法解决的非线性问题。

### 结构

一个MLP至少包含三层：

- **输入层 (Input Layer)**：接收原始输入数据。
- **隐藏层 (Hidden Layer)**：可以有一层或多层，负责进行特征转换和提取。
- **输出层 (Output Layer)**：产生最终的预测结果。

### 表达能力

- **单个感知机**：只能实现线性分类，表达能力有限。
- **单层聚合多个感知机**：可以组合多个线性边界，表达能力增强。
- **叠加多层的感知机 (MLP)**：通过组合多个隐藏层，MLP可以学习复杂的非线性函数，**表达能力非常强大**。理论上，具有足够隐藏单元的MLP可以逼近任何连续函数。

例如，通过组合两个感知机（`g1`, `g2`）和一个AND逻辑，可以构造出一个XOR函数，解决了非线性分类问题：

$$XOR(g1, g2) = OR(AND(-g1, g2), AND(g1, -g2))$$

---

## 6. 激活函数 (Activation Functions)

### 6.1 什么是激活函数？

激活函数 (Activation Function) 是神经网络中每个神经元的输出计算方法。它接收一个加权输入和（`s`），并对其进行非线性变换，然后将结果作为该神经元的输出传递给下一层。

### 6.2 为什么需要激活函数？

激活函数的主要目的是向网络中**引入非线性**。如果一个多层网络没有非线性激活函数，那么无论它有多少层，其效果都等同于一个单层的线性模型，这将极大地限制其表达能力，使其无法学习复杂的模式。

### 6.3 常见的激活函数

#### S型函数 (Sigmoid-like Functions)

- **Sigmoid函数**:

  $$f(x) = \frac{1}{1 + e^{-x}}$$

  将输入压缩到 `(0, 1)` 区间，常用于二元分类的输出层。

- **Tanh函数 (双曲正切函数)**:

  $$f(x) = \tanh(x) = \frac{2}{1 + e^{-2x}} - 1$$

  将输入压缩到 `(-1, 1)` 区间，通常比Sigmoid函数收敛更快，因为它的输出是零中心的。

#### 其他常用函数

- **ReLU函数 (Rectified Linear Unit)**:

  $$\sigma(x) = \max(0, x)$$

  当输入 `x > 0` 时，输出 `x`；当 `x <= 0` 时，输出 `0`。

  **优点**：计算简单，收敛速度快，有效缓解了梯度消失问题。是目前最流行的激活函数之一。

- **Softmax函数**:

  $$\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

  用于多类别分类问题的输出层。它可以将一个包含任意实数的K维向量"压缩"成一个值在 `(0, 1)` 范围内、元素总和为1的K维实向量，代表了每个类别的概率。

